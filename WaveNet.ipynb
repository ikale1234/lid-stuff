{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d9c111-1a42-4802-af60-4aef0cda2d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, LSTM, RepeatVector, Embedding, TimeDistributed, Dense, Dropout, Conv1D, GRU, BatchNormalization, Activation\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import ast\n",
    "from itertools import cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35ce5351-0e13-4570-a4d6-ca477f48d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bruteforce_CWE-307.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5000f609-cff3-4c18-8971-5f1db0180ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attack = []\n",
    "normal = []\n",
    "for i in range(len(df)):\n",
    "    calls = ast.literal_eval(df.iloc[i]['syscalls'])\n",
    "    check = df.iloc[i]['is_exploit']\n",
    "    \n",
    "    temp_list = []\n",
    "    for j in range(len(calls)):\n",
    "        temp_list.append(calls[j]['name'])\n",
    "    if check:\n",
    "        attack.append(temp_list)\n",
    "    else:\n",
    "        normal.append(temp_list)\n",
    "\n",
    "both_lists = attack + normal\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(both_lists)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X_train = normal[:500]\n",
    "X_val = normal[500:750]\n",
    "X_test = normal[750:] + attack\n",
    "tokened_Xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "tokened_Xtest = tokenizer.texts_to_sequences(X_test)\n",
    "tokened_Xval = tokenizer.texts_to_sequences(X_val)\n",
    "max_length = 5000\n",
    "X_train_padded = pad_sequences(tokened_Xtrain, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(tokened_Xtest, maxlen=max_length, padding='post')\n",
    "X_val_padded = pad_sequences(tokened_Xval, maxlen=max_length, padding='post')\n",
    "\n",
    "K = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20536c5-3f3e-4f71-87f6-34347739e456",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=K, output_dim=400, input_length=None))\n",
    "model.add(Conv1D(filters=16, kernel_size=3, padding=\"causal\"))\n",
    "for i, dilation_rate in zip(range(10), cycle(dilation_rates)):\n",
    "    model.add(WaveNetBlock(dilation_rate=dilation_rate))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(K, activation='softmax'))  \n",
    "model.add(Conv1D(\n",
    "        bias_initializer=\"zeros\",\n",
    "        filters=16,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_size=3,\n",
    "        padding=\"causal\",\n",
    "    ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(\n",
    "        activation=\"softmax\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        filters=K,\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_size=1,\n",
    "    ))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_crossentropy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22e7d2bf-6bc1-463b-aab0-bcc60b9db3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 2, 4, 8, 16, 32, 64, 128, 256, 512)\n"
     ]
    }
   ],
   "source": [
    "dilation_rates = tuple(2 ** x for x in range(10))\n",
    "print(dilation_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d315c1b7-bd04-4b19-83ae-7e42d91b3829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WaveNetBlock(tensorflow.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implements the basic building block of the WaveNet architecture:\n",
    "        https://arxiv.org/abs/1609.03499\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        activation=\"tanh\",\n",
    "        bias_initializer=\"zeros\",\n",
    "        dilation_rate=1,\n",
    "        filters=16,\n",
    "        gate_activation=\"sigmoid\",\n",
    "        kernel_initializer=\"glorot_uniform\",\n",
    "        kernel_size=3,\n",
    "        padding=\"causal\",\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            activation: (str or Callable)\n",
    "                Name of a keras activation function or an instance of a keras/Tensorflow activation function.\n",
    "                Applied to the non-gate branch of a gated activation unit.\n",
    "            bias_initializer: (str or Callable)\n",
    "                Name or instance of a keras.initializers.Initializer.\n",
    "            dilation_rate: (int)\n",
    "                Dilation rate used in convolutions.\n",
    "            filters: (int)\n",
    "                Number of filters used in convolutions.\n",
    "            kernel_initializer: (str or Callable)\n",
    "                Name or instance of a keras.initializers.Initializer.\n",
    "            gate_activation: (str or Callable)\n",
    "                Name of a keras activation function or an instance of a keras/Tensorflow activation function.\n",
    "                Applied to the gate branch of a gated activation unit\n",
    "            kernel_size: (tuple[int] or int)\n",
    "                Dimensions of the convolution filters.\n",
    "            residual_merge: (keras.layers.Layer)\n",
    "                Keras layer that merges the input and output branches of a residual block.\n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.bias_initializer = bias_initializer\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.filters = filters\n",
    "        self.gate_activation = gate_activation\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "\n",
    "        self.value_branch = None\n",
    "        self.gate_branch = None\n",
    "        self.skip_out = None\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.value_branch = Conv1D(\n",
    "            activation=self.activation,\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            filters=self.filters,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.value_branch.build(input_shape)\n",
    "        self._trainable_weights.extend(self.value_branch.trainable_weights)\n",
    "\n",
    "        self.gate_branch = Conv1D(\n",
    "            activation=self.gate_activation,\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            filters=self.filters,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_size=self.kernel_size,\n",
    "            padding=self.padding,\n",
    "        )\n",
    "        self.gate_branch.build(input_shape)\n",
    "        self._trainable_weights.extend(self.gate_branch.trainable_weights)\n",
    "\n",
    "        self.skip_out = Conv1D(\n",
    "            bias_initializer=self.bias_initializer,\n",
    "            dilation_rate=self.dilation_rate,\n",
    "            filters=self.filters,\n",
    "            kernel_initializer=self.kernel_initializer,\n",
    "            kernel_size=1,\n",
    "        )\n",
    "        self.skip_out.build(self.value_branch.compute_output_shape(input_shape))\n",
    "        self._trainable_weights.extend(self.skip_out.trainable_weights)\n",
    "\n",
    "        super().build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, **kwargs):\n",
    "        value = self.value_branch(inputs)\n",
    "        gate = self.gate_branch(inputs)\n",
    "        gated_value = tensorflow.keras.layers.multiply([value, gate])\n",
    "        skip_out = self.skip_out(gated_value)\n",
    "        return tensorflow.keras.layers.concatenate([inputs, skip_out])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "        output_shape[-1] += self.filters\n",
    "        return tuple(output_shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e088577-e4ef-4aaf-8e76-e2295fbd17f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_inputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(word_index) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_inputs\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_inputs' is not defined"
     ]
    }
   ],
   "source": [
    "K = len(word_index) + 1\n",
    "print(train_inputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df025cd8-1caf-4e75-863e-59b4e67e2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_train_labels(x):\n",
    "        return x[:-1], x[1:]\n",
    "K = len(word_index) + 1\n",
    "train = [add_train_labels(seq) for seq in X_train_padded]\n",
    "val = [add_train_labels(seq) for seq in X_val_padded]\n",
    "\n",
    "train_inputs, train_targets = zip(*train)\n",
    "val_inputs, val_targets = zip(*val)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_inputs = np.array(train_inputs)\n",
    "train_targets = np.array(train_targets)\n",
    "val_inputs = np.array(val_inputs)\n",
    "val_targets = np.array(val_targets)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Metric to monitor\n",
    "    patience=3,          # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore the model weights from the epoch with the best value of the monitored metric\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95f50f07-ddf3-4d71-9b43-303a64da1fe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "27/27 - 71s - loss: 0.9021 - sparse_categorical_crossentropy: 0.9021 - val_loss: 0.8187 - val_sparse_categorical_crossentropy: 0.8187 - 71s/epoch - 3s/step\n",
      "Epoch 2/150\n",
      "27/27 - 64s - loss: 0.3711 - sparse_categorical_crossentropy: 0.3711 - val_loss: 0.5402 - val_sparse_categorical_crossentropy: 0.5402 - 64s/epoch - 2s/step\n",
      "Epoch 3/150\n",
      "27/27 - 64s - loss: 0.3191 - sparse_categorical_crossentropy: 0.3191 - val_loss: 0.4125 - val_sparse_categorical_crossentropy: 0.4125 - 64s/epoch - 2s/step\n",
      "Epoch 4/150\n",
      "27/27 - 63s - loss: 0.2911 - sparse_categorical_crossentropy: 0.2911 - val_loss: 0.3408 - val_sparse_categorical_crossentropy: 0.3408 - 63s/epoch - 2s/step\n",
      "Epoch 5/150\n",
      "27/27 - 63s - loss: 0.2721 - sparse_categorical_crossentropy: 0.2721 - val_loss: 0.3028 - val_sparse_categorical_crossentropy: 0.3028 - 63s/epoch - 2s/step\n",
      "Epoch 6/150\n",
      "27/27 - 63s - loss: 0.2587 - sparse_categorical_crossentropy: 0.2587 - val_loss: 0.2802 - val_sparse_categorical_crossentropy: 0.2802 - 63s/epoch - 2s/step\n",
      "Epoch 7/150\n",
      "27/27 - 63s - loss: 0.2490 - sparse_categorical_crossentropy: 0.2490 - val_loss: 0.2653 - val_sparse_categorical_crossentropy: 0.2653 - 63s/epoch - 2s/step\n",
      "Epoch 8/150\n",
      "27/27 - 63s - loss: 0.2383 - sparse_categorical_crossentropy: 0.2383 - val_loss: 0.2554 - val_sparse_categorical_crossentropy: 0.2554 - 63s/epoch - 2s/step\n",
      "Epoch 9/150\n",
      "27/27 - 64s - loss: 0.2322 - sparse_categorical_crossentropy: 0.2322 - val_loss: 0.2468 - val_sparse_categorical_crossentropy: 0.2468 - 64s/epoch - 2s/step\n",
      "Epoch 10/150\n",
      "27/27 - 63s - loss: 0.2254 - sparse_categorical_crossentropy: 0.2254 - val_loss: 0.2399 - val_sparse_categorical_crossentropy: 0.2399 - 63s/epoch - 2s/step\n",
      "Epoch 11/150\n",
      "27/27 - 64s - loss: 0.2209 - sparse_categorical_crossentropy: 0.2209 - val_loss: 0.2354 - val_sparse_categorical_crossentropy: 0.2354 - 64s/epoch - 2s/step\n",
      "Epoch 12/150\n",
      "27/27 - 62s - loss: 0.2176 - sparse_categorical_crossentropy: 0.2176 - val_loss: 0.2310 - val_sparse_categorical_crossentropy: 0.2310 - 62s/epoch - 2s/step\n",
      "Epoch 13/150\n",
      "27/27 - 63s - loss: 0.2128 - sparse_categorical_crossentropy: 0.2128 - val_loss: 0.2290 - val_sparse_categorical_crossentropy: 0.2290 - 63s/epoch - 2s/step\n",
      "Epoch 14/150\n",
      "27/27 - 63s - loss: 0.2105 - sparse_categorical_crossentropy: 0.2105 - val_loss: 0.2240 - val_sparse_categorical_crossentropy: 0.2240 - 63s/epoch - 2s/step\n",
      "Epoch 15/150\n",
      "27/27 - 64s - loss: 0.2057 - sparse_categorical_crossentropy: 0.2057 - val_loss: 0.2202 - val_sparse_categorical_crossentropy: 0.2202 - 64s/epoch - 2s/step\n",
      "Epoch 16/150\n",
      "27/27 - 63s - loss: 0.2028 - sparse_categorical_crossentropy: 0.2028 - val_loss: 0.2172 - val_sparse_categorical_crossentropy: 0.2172 - 63s/epoch - 2s/step\n",
      "Epoch 17/150\n",
      "27/27 - 64s - loss: 0.2003 - sparse_categorical_crossentropy: 0.2003 - val_loss: 0.2145 - val_sparse_categorical_crossentropy: 0.2145 - 64s/epoch - 2s/step\n",
      "Epoch 18/150\n",
      "27/27 - 63s - loss: 0.1989 - sparse_categorical_crossentropy: 0.1989 - val_loss: 0.2127 - val_sparse_categorical_crossentropy: 0.2127 - 63s/epoch - 2s/step\n",
      "Epoch 19/150\n",
      "27/27 - 63s - loss: 0.1966 - sparse_categorical_crossentropy: 0.1966 - val_loss: 0.2109 - val_sparse_categorical_crossentropy: 0.2109 - 63s/epoch - 2s/step\n",
      "Epoch 20/150\n",
      "27/27 - 62s - loss: 0.1943 - sparse_categorical_crossentropy: 0.1943 - val_loss: 0.2078 - val_sparse_categorical_crossentropy: 0.2078 - 62s/epoch - 2s/step\n",
      "Epoch 21/150\n",
      "27/27 - 63s - loss: 0.1925 - sparse_categorical_crossentropy: 0.1925 - val_loss: 0.2054 - val_sparse_categorical_crossentropy: 0.2054 - 63s/epoch - 2s/step\n",
      "Epoch 22/150\n",
      "27/27 - 63s - loss: 0.1908 - sparse_categorical_crossentropy: 0.1908 - val_loss: 0.2044 - val_sparse_categorical_crossentropy: 0.2044 - 63s/epoch - 2s/step\n",
      "Epoch 23/150\n",
      "27/27 - 63s - loss: 0.1890 - sparse_categorical_crossentropy: 0.1890 - val_loss: 0.2029 - val_sparse_categorical_crossentropy: 0.2029 - 63s/epoch - 2s/step\n",
      "Epoch 24/150\n",
      "27/27 - 64s - loss: 0.1875 - sparse_categorical_crossentropy: 0.1875 - val_loss: 0.2018 - val_sparse_categorical_crossentropy: 0.2018 - 64s/epoch - 2s/step\n",
      "Epoch 25/150\n",
      "27/27 - 63s - loss: 0.1860 - sparse_categorical_crossentropy: 0.1860 - val_loss: 0.1997 - val_sparse_categorical_crossentropy: 0.1997 - 63s/epoch - 2s/step\n",
      "Epoch 26/150\n",
      "27/27 - 63s - loss: 0.1840 - sparse_categorical_crossentropy: 0.1840 - val_loss: 0.1986 - val_sparse_categorical_crossentropy: 0.1986 - 63s/epoch - 2s/step\n",
      "Epoch 27/150\n",
      "27/27 - 63s - loss: 0.1829 - sparse_categorical_crossentropy: 0.1829 - val_loss: 0.1968 - val_sparse_categorical_crossentropy: 0.1968 - 63s/epoch - 2s/step\n",
      "Epoch 28/150\n",
      "27/27 - 62s - loss: 0.1819 - sparse_categorical_crossentropy: 0.1819 - val_loss: 0.1949 - val_sparse_categorical_crossentropy: 0.1949 - 62s/epoch - 2s/step\n",
      "Epoch 29/150\n",
      "27/27 - 64s - loss: 0.1807 - sparse_categorical_crossentropy: 0.1807 - val_loss: 0.1935 - val_sparse_categorical_crossentropy: 0.1935 - 64s/epoch - 2s/step\n",
      "Epoch 30/150\n",
      "27/27 - 62s - loss: 0.1799 - sparse_categorical_crossentropy: 0.1799 - val_loss: 0.1925 - val_sparse_categorical_crossentropy: 0.1925 - 62s/epoch - 2s/step\n",
      "Epoch 31/150\n",
      "27/27 - 65s - loss: 0.1782 - sparse_categorical_crossentropy: 0.1782 - val_loss: 0.1907 - val_sparse_categorical_crossentropy: 0.1907 - 65s/epoch - 2s/step\n",
      "Epoch 32/150\n",
      "27/27 - 62s - loss: 0.1777 - sparse_categorical_crossentropy: 0.1777 - val_loss: 0.1902 - val_sparse_categorical_crossentropy: 0.1902 - 62s/epoch - 2s/step\n",
      "Epoch 33/150\n",
      "27/27 - 61s - loss: 0.1764 - sparse_categorical_crossentropy: 0.1764 - val_loss: 0.1880 - val_sparse_categorical_crossentropy: 0.1880 - 61s/epoch - 2s/step\n",
      "Epoch 34/150\n",
      "27/27 - 65s - loss: 0.1749 - sparse_categorical_crossentropy: 0.1749 - val_loss: 0.1869 - val_sparse_categorical_crossentropy: 0.1869 - 65s/epoch - 2s/step\n",
      "Epoch 35/150\n",
      "27/27 - 64s - loss: 0.1742 - sparse_categorical_crossentropy: 0.1742 - val_loss: 0.1865 - val_sparse_categorical_crossentropy: 0.1865 - 64s/epoch - 2s/step\n",
      "Epoch 36/150\n",
      "27/27 - 64s - loss: 0.1738 - sparse_categorical_crossentropy: 0.1738 - val_loss: 0.1871 - val_sparse_categorical_crossentropy: 0.1871 - 64s/epoch - 2s/step\n",
      "Epoch 37/150\n",
      "27/27 - 62s - loss: 0.1727 - sparse_categorical_crossentropy: 0.1727 - val_loss: 0.1848 - val_sparse_categorical_crossentropy: 0.1848 - 62s/epoch - 2s/step\n",
      "Epoch 38/150\n",
      "27/27 - 62s - loss: 0.1719 - sparse_categorical_crossentropy: 0.1719 - val_loss: 0.1847 - val_sparse_categorical_crossentropy: 0.1847 - 62s/epoch - 2s/step\n",
      "Epoch 39/150\n",
      "27/27 - 62s - loss: 0.1713 - sparse_categorical_crossentropy: 0.1713 - val_loss: 0.1846 - val_sparse_categorical_crossentropy: 0.1846 - 62s/epoch - 2s/step\n",
      "Epoch 40/150\n",
      "27/27 - 61s - loss: 0.1706 - sparse_categorical_crossentropy: 0.1706 - val_loss: 0.1824 - val_sparse_categorical_crossentropy: 0.1824 - 61s/epoch - 2s/step\n",
      "Epoch 41/150\n",
      "27/27 - 61s - loss: 0.1698 - sparse_categorical_crossentropy: 0.1698 - val_loss: 0.1836 - val_sparse_categorical_crossentropy: 0.1836 - 61s/epoch - 2s/step\n",
      "Epoch 42/150\n",
      "27/27 - 62s - loss: 0.1689 - sparse_categorical_crossentropy: 0.1689 - val_loss: 0.1816 - val_sparse_categorical_crossentropy: 0.1816 - 62s/epoch - 2s/step\n",
      "Epoch 43/150\n",
      "27/27 - 62s - loss: 0.1682 - sparse_categorical_crossentropy: 0.1682 - val_loss: 0.1802 - val_sparse_categorical_crossentropy: 0.1802 - 62s/epoch - 2s/step\n",
      "Epoch 44/150\n",
      "27/27 - 62s - loss: 0.1675 - sparse_categorical_crossentropy: 0.1675 - val_loss: 0.1793 - val_sparse_categorical_crossentropy: 0.1793 - 62s/epoch - 2s/step\n",
      "Epoch 45/150\n",
      "27/27 - 62s - loss: 0.1669 - sparse_categorical_crossentropy: 0.1669 - val_loss: 0.1799 - val_sparse_categorical_crossentropy: 0.1799 - 62s/epoch - 2s/step\n",
      "Epoch 46/150\n",
      "27/27 - 63s - loss: 0.1669 - sparse_categorical_crossentropy: 0.1669 - val_loss: 0.1786 - val_sparse_categorical_crossentropy: 0.1786 - 63s/epoch - 2s/step\n",
      "Epoch 47/150\n",
      "27/27 - 63s - loss: 0.1661 - sparse_categorical_crossentropy: 0.1661 - val_loss: 0.1774 - val_sparse_categorical_crossentropy: 0.1774 - 63s/epoch - 2s/step\n",
      "Epoch 48/150\n",
      "27/27 - 61s - loss: 0.1650 - sparse_categorical_crossentropy: 0.1650 - val_loss: 0.1774 - val_sparse_categorical_crossentropy: 0.1774 - 61s/epoch - 2s/step\n",
      "Epoch 49/150\n",
      "27/27 - 60s - loss: 0.1639 - sparse_categorical_crossentropy: 0.1639 - val_loss: 0.1762 - val_sparse_categorical_crossentropy: 0.1762 - 60s/epoch - 2s/step\n",
      "Epoch 50/150\n",
      "27/27 - 61s - loss: 0.1639 - sparse_categorical_crossentropy: 0.1639 - val_loss: 0.1767 - val_sparse_categorical_crossentropy: 0.1767 - 61s/epoch - 2s/step\n",
      "Epoch 51/150\n",
      "27/27 - 60s - loss: 0.1640 - sparse_categorical_crossentropy: 0.1640 - val_loss: 0.1755 - val_sparse_categorical_crossentropy: 0.1755 - 60s/epoch - 2s/step\n",
      "Epoch 52/150\n",
      "27/27 - 60s - loss: 0.1631 - sparse_categorical_crossentropy: 0.1631 - val_loss: 0.1754 - val_sparse_categorical_crossentropy: 0.1754 - 60s/epoch - 2s/step\n",
      "Epoch 53/150\n",
      "27/27 - 60s - loss: 0.1623 - sparse_categorical_crossentropy: 0.1623 - val_loss: 0.1741 - val_sparse_categorical_crossentropy: 0.1741 - 60s/epoch - 2s/step\n",
      "Epoch 54/150\n",
      "27/27 - 60s - loss: 0.1618 - sparse_categorical_crossentropy: 0.1618 - val_loss: 0.1736 - val_sparse_categorical_crossentropy: 0.1736 - 60s/epoch - 2s/step\n",
      "Epoch 55/150\n",
      "27/27 - 59s - loss: 0.1614 - sparse_categorical_crossentropy: 0.1614 - val_loss: 0.1732 - val_sparse_categorical_crossentropy: 0.1732 - 59s/epoch - 2s/step\n",
      "Epoch 56/150\n",
      "27/27 - 60s - loss: 0.1607 - sparse_categorical_crossentropy: 0.1607 - val_loss: 0.1729 - val_sparse_categorical_crossentropy: 0.1729 - 60s/epoch - 2s/step\n",
      "Epoch 57/150\n",
      "27/27 - 59s - loss: 0.1602 - sparse_categorical_crossentropy: 0.1602 - val_loss: 0.1733 - val_sparse_categorical_crossentropy: 0.1733 - 59s/epoch - 2s/step\n",
      "Epoch 58/150\n",
      "27/27 - 60s - loss: 0.1595 - sparse_categorical_crossentropy: 0.1595 - val_loss: 0.1719 - val_sparse_categorical_crossentropy: 0.1719 - 60s/epoch - 2s/step\n",
      "Epoch 59/150\n",
      "27/27 - 60s - loss: 0.1592 - sparse_categorical_crossentropy: 0.1592 - val_loss: 0.1707 - val_sparse_categorical_crossentropy: 0.1707 - 60s/epoch - 2s/step\n",
      "Epoch 60/150\n",
      "27/27 - 60s - loss: 0.1592 - sparse_categorical_crossentropy: 0.1592 - val_loss: 0.1712 - val_sparse_categorical_crossentropy: 0.1712 - 60s/epoch - 2s/step\n",
      "Epoch 61/150\n",
      "27/27 - 60s - loss: 0.1587 - sparse_categorical_crossentropy: 0.1587 - val_loss: 0.1709 - val_sparse_categorical_crossentropy: 0.1709 - 60s/epoch - 2s/step\n",
      "Epoch 62/150\n",
      "27/27 - 59s - loss: 0.1591 - sparse_categorical_crossentropy: 0.1591 - val_loss: 0.1717 - val_sparse_categorical_crossentropy: 0.1717 - 59s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2e2403d00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "            train_inputs,\n",
    "            train_targets,\n",
    "            validation_data=(val_inputs, val_targets),\n",
    "            epochs=150,\n",
    "            verbose=2,\n",
    "            shuffle=True,\n",
    "            callbacks=[early_stopping],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "327b0947-375d-4d2d-8115-d209cf1c9845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 4s 426ms/step\n",
      "622.1158905029297\n"
     ]
    }
   ],
   "source": [
    "val = X_val_padded\n",
    "y_val = model.predict(val)\n",
    "sums_val = np.array([-np.log(pred.max(axis=-1)).sum(axis=-1) for pred in y_val])\n",
    "threshold = np.mean(sums_val) + 2*np.std(sums_val)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "231cce41-8d01-4d52-8c90-e9460091b4af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n",
      "11/11 [==============================] - 5s 434ms/step\n"
     ]
    }
   ],
   "source": [
    "test = X_test_padded\n",
    "print(len(test))\n",
    "y_pred = model.predict(test)\n",
    "sums = np.array([-np.log(pred.max(axis=-1)).sum(axis=-1) for pred in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e91b1d33-1580-4f66-98b4-09c5f748151e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323\n",
      "342\n",
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "threshold = np.mean(sums_val) + 2*np.std(sums_val)\n",
    "successes = 0\n",
    "total = len(test)\n",
    "pred = []\n",
    "norms_len = total - len(attack)\n",
    "testY = []\n",
    "for i in range(total):\n",
    "    if sums[i] <= threshold:\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)\n",
    "    if i < norms_len:\n",
    "        testY.append(0)\n",
    "        if sums[i] <= threshold:\n",
    "            successes += 1\n",
    "    else:\n",
    "        testY.append(1)\n",
    "        if sums[i] > threshold:\n",
    "            successes += 1\n",
    "print(successes)\n",
    "print(total)\n",
    "print(successes / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "73421f60-e9cf-414e-9d02-51412686438e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74df3759-45aa-4193-8ec8-9e9b0c646e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.9554655870445344\n",
      "Recall:  0.9672131147540983\n",
      "f1:  0.9613034623217923\n"
     ]
    }
   ],
   "source": [
    "precision = precision_score(testY, pred, pos_label=0)\n",
    "recall = recall_score(testY, pred, pos_label=0)\n",
    "f1 = f1_score(testY, pred, pos_label=0)\n",
    "print(\"Precision: \",precision)\n",
    "print(\"Recall: \",recall)\n",
    "print(\"f1: \",f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9170747c-08ba-4c24-a597-cb5562c79c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('CVE-2012-2122.csv')\n",
    "attack = []\n",
    "normal = []\n",
    "for i in range(len(df2)):\n",
    "    calls = ast.literal_eval(df2.iloc[i]['syscalls'])\n",
    "    check = df2.iloc[i]['is_exploit']\n",
    "    \n",
    "    temp_list = []\n",
    "    for j in range(len(calls)):\n",
    "        temp_list.append(calls[j]['name'])\n",
    "    if check:\n",
    "        attack.append(temp_list)\n",
    "    else:\n",
    "        normal.append(temp_list)\n",
    "\n",
    "both_lists = attack + normal\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(both_lists)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "X_train = normal[:850]\n",
    "X_val = normal[850:1050]\n",
    "X_test = normal[1050:] + attack\n",
    "tokened_Xtrain = tokenizer.texts_to_sequences(X_train)\n",
    "tokened_Xtest = tokenizer.texts_to_sequences(X_test)\n",
    "tokened_Xval = tokenizer.texts_to_sequences(X_val)\n",
    "max_length = 7500\n",
    "X_train_padded = pad_sequences(tokened_Xtrain, maxlen=max_length, padding='post')\n",
    "X_test_padded = pad_sequences(tokened_Xtest, maxlen=max_length, padding='post')\n",
    "X_val_padded = pad_sequences(tokened_Xval, maxlen=max_length, padding='post')\n",
    "\n",
    "K = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cff9528-b3d2-473a-a0c3-49974cbb589f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "16/16 - 25s - loss: 0.2098 - sparse_categorical_crossentropy: 0.2098 - val_loss: 0.2238 - val_sparse_categorical_crossentropy: 0.2238 - 25s/epoch - 2s/step\n",
      "Epoch 2/150\n",
      "16/16 - 24s - loss: 0.2092 - sparse_categorical_crossentropy: 0.2092 - val_loss: 0.2240 - val_sparse_categorical_crossentropy: 0.2240 - 24s/epoch - 2s/step\n",
      "Epoch 3/150\n",
      "16/16 - 24s - loss: 0.2086 - sparse_categorical_crossentropy: 0.2086 - val_loss: 0.2231 - val_sparse_categorical_crossentropy: 0.2231 - 24s/epoch - 2s/step\n",
      "Epoch 4/150\n",
      "16/16 - 25s - loss: 0.2081 - sparse_categorical_crossentropy: 0.2081 - val_loss: 0.2239 - val_sparse_categorical_crossentropy: 0.2239 - 25s/epoch - 2s/step\n",
      "Epoch 5/150\n",
      "16/16 - 24s - loss: 0.2074 - sparse_categorical_crossentropy: 0.2074 - val_loss: 0.2226 - val_sparse_categorical_crossentropy: 0.2226 - 24s/epoch - 2s/step\n",
      "Epoch 6/150\n",
      "16/16 - 25s - loss: 0.2064 - sparse_categorical_crossentropy: 0.2064 - val_loss: 0.2221 - val_sparse_categorical_crossentropy: 0.2221 - 25s/epoch - 2s/step\n",
      "Epoch 7/150\n",
      "16/16 - 24s - loss: 0.2059 - sparse_categorical_crossentropy: 0.2059 - val_loss: 0.2221 - val_sparse_categorical_crossentropy: 0.2221 - 24s/epoch - 2s/step\n",
      "Epoch 8/150\n",
      "16/16 - 24s - loss: 0.2054 - sparse_categorical_crossentropy: 0.2054 - val_loss: 0.2222 - val_sparse_categorical_crossentropy: 0.2222 - 24s/epoch - 2s/step\n",
      "Epoch 9/150\n",
      "16/16 - 24s - loss: 0.2051 - sparse_categorical_crossentropy: 0.2051 - val_loss: 0.2207 - val_sparse_categorical_crossentropy: 0.2207 - 24s/epoch - 2s/step\n",
      "Epoch 10/150\n",
      "16/16 - 24s - loss: 0.2047 - sparse_categorical_crossentropy: 0.2047 - val_loss: 0.2203 - val_sparse_categorical_crossentropy: 0.2203 - 24s/epoch - 2s/step\n",
      "Epoch 11/150\n",
      "16/16 - 24s - loss: 0.2044 - sparse_categorical_crossentropy: 0.2044 - val_loss: 0.2220 - val_sparse_categorical_crossentropy: 0.2220 - 24s/epoch - 2s/step\n",
      "Epoch 12/150\n",
      "16/16 - 24s - loss: 0.2041 - sparse_categorical_crossentropy: 0.2041 - val_loss: 0.2191 - val_sparse_categorical_crossentropy: 0.2191 - 24s/epoch - 2s/step\n",
      "Epoch 13/150\n",
      "16/16 - 24s - loss: 0.2036 - sparse_categorical_crossentropy: 0.2036 - val_loss: 0.2208 - val_sparse_categorical_crossentropy: 0.2208 - 24s/epoch - 2s/step\n",
      "Epoch 14/150\n",
      "16/16 - 24s - loss: 0.2027 - sparse_categorical_crossentropy: 0.2027 - val_loss: 0.2194 - val_sparse_categorical_crossentropy: 0.2194 - 24s/epoch - 2s/step\n",
      "Epoch 15/150\n",
      "16/16 - 24s - loss: 0.2022 - sparse_categorical_crossentropy: 0.2022 - val_loss: 0.2191 - val_sparse_categorical_crossentropy: 0.2191 - 24s/epoch - 2s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2e70c6f20>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "            train_inputs,\n",
    "            train_targets,\n",
    "            validation_data=(val_inputs, val_targets),\n",
    "            epochs=150,\n",
    "            verbose=2,\n",
    "            shuffle=True,\n",
    "            callbacks=[early_stopping],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4446fa53-19ad-4515-9226-6877a130fc9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 5s 647ms/step\n",
      "1111.7434692382812\n"
     ]
    }
   ],
   "source": [
    "val = X_val_padded\n",
    "y_val = model.predict(val)\n",
    "sums_val = np.array([-np.log(pred.max(axis=-1)).sum(axis=-1) for pred in y_val])\n",
    "threshold = np.mean(sums_val) + 2*np.std(sums_val)\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f84a631-e12c-4bde-abbc-fb0170538d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "11/11 [==============================] - 8s 690ms/step\n"
     ]
    }
   ],
   "source": [
    "test = X_test_padded\n",
    "print(len(test))\n",
    "y_pred = model.predict(test)\n",
    "sums = np.array([-np.log(pred.max(axis=-1)).sum(axis=-1) for pred in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45d31f47-6835-4886-ae1e-d0fcafbc21a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262\n",
      "345\n",
      "0.7594202898550725\n"
     ]
    }
   ],
   "source": [
    "threshold = np.mean(sums_val) + np.std(sums_val)\n",
    "successes = 0\n",
    "total = len(test)\n",
    "pred = []\n",
    "norms_len = total - len(attack)\n",
    "testY = []\n",
    "for i in range(total):\n",
    "    if sums[i] <= threshold:\n",
    "        pred.append(0)\n",
    "    else:\n",
    "        pred.append(1)\n",
    "    if i < norms_len:\n",
    "        testY.append(0)\n",
    "        if sums[i] <= threshold:\n",
    "            successes += 1\n",
    "    else:\n",
    "        testY.append(1)\n",
    "        if sums[i] > threshold:\n",
    "            successes += 1\n",
    "print(successes)\n",
    "print(total)\n",
    "print(successes / total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a48c24-cd17-4e3d-9e36-242b56fd0291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
